# -*- coding: utf-8 -*-
"""(0242220005101691)regression and classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AUfCSnwW5IMWTXFMhJm-2iomBcj-sw1V
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

#Loading the dataset
df=pd.read_csv('/content/drive/MyDrive/data/education_career_success.csv')

#For displaying basic information
print("Dataset information : ")
df.info()

#For displaying first five rows
print("First five rows")
df.head().T

from google.colab import drive
drive.mount('/content/drive')

df.head().T

#For checking missing values
print("Missing values per column : ")
print(df.isnull().sum())

#Label encoding(use it when the lebel can be ranked example:poor<average<good)
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
df['Current_Job_Level'] = label_encoder.fit_transform(df['Current_Job_Level'])#in df['Current_Job_Leve']write encoded job level then run it again

print("Original values:", df['Current_Job_Level'].unique())
print("Encoded values:", df['Current_Job_Level'].unique())
df[['Current_Job_Level', 'Current_Job_Level']].head()

df.head().T

#changing Yes into 1 and no into 0 in the Entrepreneurship column
df['Entrepreneurship'] = df['Entrepreneurship'].map({'Yes': 1, 'No': 0})

#Use one hot encoding when there is no superiority in lebels
onehot_encoded = pd.get_dummies(df['Gender'], prefix='Gender')
df = df.drop('Gender', axis=1) #dropping the original column
df = pd.concat([df, onehot_encoded], axis=1)

print("One-Hot Encoded columns:")
print(onehot_encoded.head())

#Use one hot encoding when there is no superiority in lebels
onehot_encoded = pd.get_dummies(df['Field_of_Study'], prefix='Field_of_Study')
df = df.drop('Field_of_Study', axis=1)
df = pd.concat([df, onehot_encoded], axis=1)

print("One-Hot Encoded columns:")
print(onehot_encoded.head())

df.head().T

"""outliers detection"""

def detect_outliers_iqr(data, column):
    """Detect outliers using IQR method"""
    Q1 = data[column].quantile(0.25)
    Q3 = data[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]
    return outliers

# Detect outliers in numeric columns
numeric_cols = df.select_dtypes(include=[np.number]).columns
for col in numeric_cols:
    outliers = detect_outliers_iqr(df, col)
    print(f"Outliers in {col} (IQR method): {len(outliers)}")

# Remove outliers using IQR
def remove_outliers_iqr(data, column):
    Q1 = data[column].quantile(0.25)
    Q3 = data[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    return data[(data[column] >= lower_bound) & (data[column] <= upper_bound)]

df_no_outliers_iqr = df.copy()
for col in numeric_cols:
    df_no_outliers_iqr = remove_outliers_iqr(df_no_outliers_iqr, col)

print(f"Original shape: {df.shape}")
print(f"Shape after IQR outlier removal: {df_no_outliers_iqr.shape}")

df.head(100).T

#standardization for "High school gpa" -------------------------------------------------------------------------------------->(mean=0,standard deviation=1)
from sklearn.preprocessing import StandardScaler

standard_scaler = StandardScaler()
df['High_School_GPA'] = standard_scaler.fit_transform(df[['High_School_GPA']])

print("Standardized values:")
df[['High_School_GPA']].head()

#standardization for "University gpa"
from sklearn.preprocessing import StandardScaler

standard_scaler = StandardScaler()
df['University_GPA'] = standard_scaler.fit_transform(df[['University_GPA']])

print("Standardized values:")
df[['University_GPA']].head()

#standardization for "SAT_Score"
from sklearn.preprocessing import StandardScaler

standard_scaler = StandardScaler()
df['SAT_Score'] = standard_scaler.fit_transform(df[['SAT_Score']])

print("Standardized values:")
df[['SAT_Score']].head()

#standardization for "Starting_Salary"
from sklearn.preprocessing import StandardScaler

standard_scaler = StandardScaler()
df['Starting_Salary'] = standard_scaler.fit_transform(df[['Starting_Salary']])

print("Standardized values:")
df[['Starting_Salary']].head()

#standardization for "Career_Satisfaction"
from sklearn.preprocessing import StandardScaler

standard_scaler = StandardScaler()
df['Career_Satisfaction'] = standard_scaler.fit_transform(df[['Career_Satisfaction']])

print("Standardized values:")
df[['Career_Satisfaction']].head()

#normalization (0-1)
from sklearn.preprocessing import MinMaxScaler

min_max_scaler = MinMaxScaler()
df['Age'] = min_max_scaler.fit_transform(df[['Age']])
print("Original vs Min-Max Scaled values:")
df[['Age']].head()

#normalization (0-1)
from sklearn.preprocessing import MinMaxScaler

min_max_scaler = MinMaxScaler()
df['Work_Life_Balance'] = min_max_scaler.fit_transform(df[['Work_Life_Balance']])
print("Original vs Min-Max Scaled values:")
df[['Work_Life_Balance']].head()

#normalization (0-1)
from sklearn.preprocessing import MinMaxScaler

min_max_scaler = MinMaxScaler()
df['Soft_Skills_Score'] = min_max_scaler.fit_transform(df[['Soft_Skills_Score']])
print("Original vs Min-Max Scaled values:")
df[['Soft_Skills_Score']].head()

#normalization (0-1)
from sklearn.preprocessing import MinMaxScaler

min_max_scaler = MinMaxScaler()
df['Networking_Score'] = min_max_scaler.fit_transform(df[['Networking_Score']])
print("Original vs Min-Max Scaled values:")
df[['Networking_Score']].head()

"""#use regression if the value you want to predict is continuous and use classification if the value you want to predict is discrete"""

#regression
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
#from sklearn.neural_network import MLPRegressor
from math import sqrt

df.head().T

"""selecting the input and output features of  the regression **task\**"""

features=['Age','High_School_GPA','SAT_Score','University_Ranking','University_GPA','Internships_Completed','Projects_Completed','Certifications','Soft_Skills_Score','Networking_Score','Job_Offers','Career_Satisfaction','Years_to_Promotion','Current_Job_Level','Work_Life_Balance','Entrepreneurship','Gender_Female','Gender_Male','Gender_Other','Field_of_Study_Arts','Field_of_Study_Business','Field_of_Study_Computer Science','Field_of_Study_Engineering']

target=['Starting_Salary']

df.isnull().any()

X = df[features]
y = df[target]

"""Perform train test split"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=200)

"""Fit on train set"""

regressor = LinearRegression()
regressor.fit(X_train, y_train)

"""Predicting on test data"""

y_prediction = regressor.predict(X_test)
print(y_prediction[:5])
print('*'*40)
print(y_test[:5])

y_test.describe()

#by using root mean square error evaluating the accuracy of the linear regression
RMSE = sqrt(mean_squared_error(y_true=y_test, y_pred=y_prediction))
print(RMSE)

#Discition tree
regressor = DecisionTreeRegressor(max_depth=50)
regressor.fit(X_train, y_train)

#predicting
y_prediction = regressor.predict(X_test)
y_prediction[:5]

y_test[:5]

RMSE = sqrt(mean_squared_error(y_true=y_test, y_pred=y_prediction))

print(RMSE)

"""Classification model"""

from sklearn.tree import DecisionTreeClassifier

data_classifier = df.copy()

data_classifier.head().T

features=['Age','High_School_GPA','SAT_Score','University_Ranking','University_GPA','Internships_Completed','Projects_Completed','Certifications','Soft_Skills_Score','Networking_Score','Job_Offers','Starting_Salary','Career_Satisfaction','Years_to_Promotion','Current_Job_Level','Work_Life_Balance','Gender_Female','Gender_Male','Gender_Other','Field_of_Study_Arts','Field_of_Study_Business','Field_of_Study_Computer Science','Field_of_Study_Engineering']
target_classifier = ['Entrepreneurship']

X = data_classifier[features]
y = data_classifier[target_classifier]

#train and split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=324)

#fit and train set
Entrepreneurship_classifier = DecisionTreeClassifier(max_leaf_nodes=20, random_state=0)

Entrepreneurship_classifier.fit(X_train, y_train)

prediction = Entrepreneurship_classifier.predict(X_test)
print(prediction[:5])
print('*'*10)
print(y_test['Entrepreneurship'][:5])

#measuring the accuracy of the classifier
accuracy_score(y_true=y_test, y_pred=prediction)

"""Logistic regression"""

from sklearn.linear_model import LogisticRegression

data_classifier.head()

#selecting input and output features for classification task
features=['Age','High_School_GPA','SAT_Score','University_Ranking','University_GPA','Internships_Completed','Projects_Completed','Certifications','Soft_Skills_Score','Networking_Score','Job_Offers','Starting_Salary','Career_Satisfaction','Years_to_Promotion','Current_Job_Level','Work_Life_Balance','Gender_Female','Gender_Male','Gender_Other','Field_of_Study_Arts','Field_of_Study_Business','Field_of_Study_Computer Science','Field_of_Study_Engineering']
target_classifier = ['Entrepreneurship']

X = data_classifier[features]
y = data_classifier[target_classifier]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=324)

logistic_regression = LogisticRegression()
logistic_regression.fit(X_train, y_train)

prediction = logistic_regression.predict(X_test)
print(prediction[:5])
print(y_test[:5])

accuracy_score(y_true=y_test, y_pred=prediction)

"""saving and loading data in local machine.........concept or data drift(throught time a word meaning changes).to solve this problem we train the model after a certain time....
[joblib] easy to use but slow....[pkl] file extention,pickle.....streamlet is a frame work for front end

"""

import pickle
#save model
with open('model_pickle_for_logistic_R.pkl','wb') as f:
  pickle.dump(logistic_regression,f)

with open('model_pickle_for_logistic_R.pkl','rb') as f:
  loaded_model=pickle.load(f)

predictions =loaded_model.predict(X_test)
print(predictions)

"""# SVM classification algorithm"""

#df.drop(columns=['Student_ID'])

x=df.drop(['Entrepreneurship','Student_ID'],axis=1)

y=df['Entrepreneurship']#assignning the y axis with the value that was dropped

from sklearn.model_selection import train_test_split
from sklearn.svm import SVC

xtrain, xtest, ytrain, ytest = train_test_split(x,y,test_size=.25,random_state=1)#specify how much data to train and test

model=SVC()#need to create an object to run the model

model.fit(xtrain,ytrain)

model.score(xtest,ytest)

"""# KNN"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

df.head().T

#x=df.drop(['Student_ID'],axis=1)
x= df.iloc[:, 1:-1].values
y= df.iloc[:, 17].values  #17 th column is gender female

import sklearn
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.20)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(x_train)

x_train = scaler.transform(x_train)
x_test = scaler.transform(x_test)

from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors=5)
classifier.fit(x_train,y_train)

y_prediction = classifier.predict(x_test)
print('prediction = ',y_prediction)

import sklearn
from sklearn.metrics import classification_report,confusion_matrix,accuracy_score
accuracy=accuracy_score(y_test,y_prediction)
print('Accuracy = ',accuracy)

"""# Naive **bayes:**"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB

x= df.drop(['Student_ID','Entrepreneurship'],axis=1)
y= df['Entrepreneurship']

x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.20)

model = GaussianNB()
model.fit(x_train,y_train)

y_prediction = model.predict(x_test)
print('prediction = ',y_prediction)

accuracy=accuracy_score(y_test,y_prediction)
print('Accuracy = ',accuracy)

df.shape

"""hyoer perameter tuning is an important topic learn it,streamlet

# dicision tree
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import export_graphviz
from sklearn.metrics import accuracy_score

#X= df.drop(['Student_ID','Entrepreneurship'],axis=1)
#y= df['Entrepreneurship']
#Xtrain,Xtest,ytrain,ytest = train_test_split(X,y,test_size=0.20)

x= df.iloc[:, 1:-1].values
y= df.iloc[:, 17].values
xtrain,xtest,ytrain,ytest = train_test_split(x,y,test_size=0.20)

model = DecisionTreeClassifier(random_state=42) # Add parameters like max_depth, criterion, etc.
model.fit(xtrain,ytrain)

y_pred = model.predict(xtest)
print('prediction = ',y_pred)

from sklearn import tree
plt.figure(figsize=(15,10))
tree.plot_tree(model,filled=True)

accuracy=accuracy_score(ytest,y_pred)
print('Accuracy = ',accuracy)

import joblib

joblib.dump(model,'model.pkl')

!python --version

"""clustering

# K-means
"""

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
import sklearn.cluster as KMeans

df.head().T

df.shape

import matplotlib.pyplot as plt
plt.scatter(df['University_GPA'],df['Networking_Score'])

df.info()

"""choosing university cgpa and internships completed colum"""

X = df.iloc[:, [5, 10]].values

print(X)

"""choosing the number of clusters
wcss[whithing cluster sum of squeres]

"""

from sklearn.cluster import KMeans
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)
    kmeans.fit(X)
    wcss.append(kmeans.inertia_)

#plot elbow graph
sns.set()
plt.plot(range(1, 11), wcss)
plt.title('The Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()

#optimum number of  cluster = 6 because less chnages are showing after that point
kmeans = KMeans(n_clusters = 3, init = 'k-means++', random_state = 42)
Y = kmeans.fit_predict(X)
print(Y)

#plotting all the clusters and their centroids
plt.figure(figsize=(8,8))
plt.scatter(X[Y == 0, 0], X[Y == 0, 1], s = 100, c = 'red', label = 'Cluster 1')
plt.scatter(X[Y == 1, 0], X[Y == 1, 1], s = 100, c = 'blue', label = 'Cluster 2')
plt.scatter(X[Y == 2, 0], X[Y == 2, 1], s = 100, c = 'green', label = 'Cluster 3')
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 100, c = 'black', label = 'Centroids')

from sklearn.datasets import make_blobs

centroids = [(-5,-5,5),(5,5,-5),(3.5,-2.5,4),(-2.5,2.5,-4)]
cluster_std = [1,1,1,1]

X,Y = make_blobs(n_samples=200,cluster_std=cluster_std,centers=centroids,n_features=3,random_state=1)

X

import plotly.express as px
fig = px.scatter_3d(x=X[:,0], y=X[:,1], z=X[:,2])
fig.show()

"""agglomerative -clustering

"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import numpy as np

data = df.iloc[:, 5:10].values

data

import scipy.cluster.hierarchy as shc

plt.figure(figsize=(10, 7))
plt.title("Customer Dendograms")
dend = shc.dendrogram(shc.linkage(data, method='ward'))

from sklearn.cluster import AgglomerativeClustering

cluster = AgglomerativeClustering(n_clusters=5, linkage='ward')
labels_=cluster.fit_predict(data)

labels_

plt.figure(figsize=(10, 7))
plt.scatter(data[:,0], data[:,2], c=cluster.labels_, cmap='rainbow')